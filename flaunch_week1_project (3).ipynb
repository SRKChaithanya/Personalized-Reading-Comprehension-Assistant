{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **<div align=\"center\">Personalized Reading Comprehension Assistant</div>**\n",
        "* **Team ID:** `214g1a3380@srit.ac.in`\n",
        "* **Members Details**:\n",
        "  * 1. **Name:** Rama Krishna Chaithanya Seela       \n",
        "        **Mail ID:** `214g1a3380@srit.ac.in`\n",
        "  * 2. **Name:** Venkata Sai Kumar Vaileti       \n",
        "      **Mail ID:** `214g1a33c0@srit.ac.in`\n",
        "\n"
      ],
      "metadata": {
        "id": "8wqatvNjJ-VT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement:  \n",
        "Students face challenges in understanding complex texts\n",
        "and often struggle with reading comprehension, affecting their academic performance.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Solution:\n",
        "Creating a reading comprehension tool using LLaMA that generates\n",
        "questions, provides summaries, and explains texts tailored to each student's reading level."
      ],
      "metadata": {
        "id": "jZPmwKOwLMSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required Installations"
      ],
      "metadata": {
        "id": "ss1h0zYp9kVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install InstructorEmbedding\n",
        "!pip install sentence_transformers\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu\n",
        "!pip install markdown\n",
        "!pip install pdfminer.six\n",
        "!pip install langchain==0.2.11 langchain-community==0.2.10 langchain-core==0.2.26 langchain-google-community==1.0.7 langchain-google-vertexai==1.0.8 langchain-text-splitters==0.2.1\n",
        "!pip install gradio PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0a-aoZt8Miv",
        "outputId": "1fde7b74-c4fa-489b-d9df-d087da6ba61f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: InstructorEmbedding in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.19.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: sentence-transformers==2.2.2 in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.19.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.19.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (3.7)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.10/dist-packages (20240706)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Requirement already satisfied: langchain==0.2.11 in /usr/local/lib/python3.10/dist-packages (0.2.11)\n",
            "Requirement already satisfied: langchain-community==0.2.10 in /usr/local/lib/python3.10/dist-packages (0.2.10)\n",
            "Requirement already satisfied: langchain-core==0.2.26 in /usr/local/lib/python3.10/dist-packages (0.2.26)\n",
            "Requirement already satisfied: langchain-google-community==1.0.7 in /usr/local/lib/python3.10/dist-packages (1.0.7)\n",
            "Requirement already satisfied: langchain-google-vertexai==1.0.8 in /usr/local/lib/python3.10/dist-packages (1.0.8)\n",
            "Requirement already satisfied: langchain-text-splitters==0.2.1 in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (4.0.3)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (0.1.129)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (8.3.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.2.10) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core==0.2.26) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core==0.2.26) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core==0.2.26) (4.12.2)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.17.1 in /usr/local/lib/python3.10/dist-packages (from langchain-google-community==1.0.7) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client<3.0.0,>=2.122.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-community==1.0.7) (2.137.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.62.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-community==1.0.7) (1.64.1)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.56.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-vertexai==1.0.8) (1.67.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.17.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-vertexai==1.0.8) (2.18.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.11.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10) (0.9.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=2.17.1->langchain-google-community==1.0.7) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=2.17.1->langchain-google-community==1.0.7) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=2.17.1->langchain-google-community==1.0.7) (1.24.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=2.17.1->langchain-google-community==1.0.7) (2.27.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community==1.0.7) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community==1.0.7) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community==1.0.7) (4.1.1)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai==1.0.8) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai==1.0.8) (1.12.5)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai==1.0.8) (2.0.6)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai==1.0.8) (0.16)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai==1.0.8) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai==1.0.8) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0,>=2.17.0->langchain-google-vertexai==1.0.8) (1.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.2.26) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.11) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.11) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.2.11) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.2.11) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.11) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.11) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.11) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.11) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.11) (3.1.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai==1.0.8) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.17.1->langchain-google-community==1.0.7) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.17.1->langchain-google-community==1.0.7) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.17.1->langchain-google-community==1.0.7) (4.9)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai==1.0.8) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai==1.0.8) (0.13.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client<3.0.0,>=2.122.0->langchain-google-community==1.0.7) (3.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.11) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.11) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.11) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.11) (0.14.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10) (1.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=2.17.1->langchain-google-community==1.0.7) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai==1.0.8) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.11) (1.2.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.44.0)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.8)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.31.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Necessary Imports"
      ],
      "metadata": {
        "id": "QoJcg8Wy9rOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "import gradio as gr\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from PyPDF2 import PdfReader\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import (\n",
        "    StreamingStdOutCallbackHandler\n",
        ")\n"
      ],
      "metadata": {
        "id": "NhVYtZHB8jbI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the embedding model\n",
        "\n",
        "* **sentence-transformers/paraphrase-MiniLM-L6-v2:** It's a pre-trained sentence-transformer model designed for generating embeddings (numerical representations) of text. Specifically, it uses paraphrase detection and semantic search tasks.\n",
        "\n",
        "* **MiniLM:** Refers to Microsoft's MiniLM model, which is a smaller, faster version of transformers like BERT. It's efficient for embedding tasks while maintaining good performance.\n",
        "\n",
        "* This model is commonly used in NLP tasks like similarity searches paraphrasing, or finding semantically related sentences.\n",
        "\n"
      ],
      "metadata": {
        "id": "rzlxLOCD9wdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Intialize_Model():\n",
        "\n",
        "    model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
        "    model_kwargs = {'device': 'cuda'}\n",
        "    encode_kwargs = {'normalize_embeddings': True}\n",
        "    hf_embedding = HuggingFaceInstructEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs\n",
        "    )\n"
      ],
      "metadata": {
        "id": "58wtYMVh5j_a"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing Files:\n",
        "\n",
        "* **chunk_size:** Each document will be split into chunks of 1200 characters.\n",
        "* **chunk_overlap:** Consecutive chunks will overlap by 300 characters. This overlap ensures that the content in each chunk has some context from the previous one.\n",
        "* **all_docs:** A list to store all the processed chunks for each document.\n",
        "* **allowed_extensions:** A list of file extensions that the function will process. Only DOCX, PDF and TXT files will be processed.\n",
        "\n",
        "* _**Extraction:**_ The **extract_text_from_file()** function determines the file type (PDF, text, docx) and extracts text accordingly. It handles PDFs using the PyPDF2 library, reads .txt files directly, and can be extended to support other formats like .docx and .md. The function incorporates error handling with try-except blocks, checks if the file exists, and returns appropriate messages for unsupported file formats or extraction errors. This approach ensures robustness when working with different file types and provides clear feedback in case of any issues during text extraction.\n",
        "\n",
        "* The text is split into chunks using the **RecursiveCharacterTextSplitter**. This method is useful when working with large text data, making it easier to process and manage the content. The chunks will be 1200 characters long, with 300-character overlaps between them.\n",
        "* Each chunk gets metadata associated with it:\n",
        "    * File Name: The name of the file (without the extension).\n",
        "    * Chunk Number: The sequence number of the chunk (1st, 2nd, etc.).\n",
        "* A **header** is created for each chunk that includes the file name and metadata (like chunk number). The header is concatenated with the actual chunk content. Each chunk with the header is added to the all_docs list.\n",
        "* The function returns the list all_docs, which contains the text chunks for all the files, along with their associated headers and metadata."
      ],
      "metadata": {
        "id": "Pe48zctMAInM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(file_path)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF file: {e}\"\n",
        "\n",
        "def extract_text_from_file(file_path):\n",
        "    \"\"\"Extract text from a file based on its extension.\"\"\"\n",
        "    try:\n",
        "        # Check if the file exists\n",
        "        if not os.path.isfile(file_path):\n",
        "            return \"File not found.\"\n",
        "\n",
        "        # Extract text from PDF files\n",
        "        if file_path.endswith('.pdf'):\n",
        "            return extract_text_from_pdf(file_path)\n",
        "\n",
        "        # Extract text from text files\n",
        "        elif file_path.endswith('.txt'):\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                return file.read()\n",
        "\n",
        "        # Add support for other formats (e.g., .docx, .md)\n",
        "        elif file_path.endswith('.docx'):\n",
        "            from docx import Document\n",
        "            doc = Document(file_path)\n",
        "            return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "        elif file_path.endswith('.md'):\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                return file.read()\n",
        "\n",
        "        else:\n",
        "            return \"Unsupported file format.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting text from file: {e}\"\n"
      ],
      "metadata": {
        "id": "xeu2_KcFBcQw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def process(files, question):\n",
        "    chunk_size = 1200\n",
        "    chunk_overlap = 300\n",
        "\n",
        "    # List to store all document chunks\n",
        "    all_docs = []\n",
        "    allowed_extensions = ['.docx.', '.pdf', '.txt']\n",
        "\n",
        "    all_docs = []\n",
        "    for filename in files:\n",
        "        # Get the file extension\n",
        "        _, file_extension = os.path.splitext(filename)\n",
        "        if file_extension in allowed_extensions:\n",
        "            file_path = os.path.join(root, filename)  # Full path of the file\n",
        "\n",
        "            # Remove the \".docx\", \".pdf\" or \".txt\" extension from the file name\n",
        "            file_name_without_extension = os.path.splitext(filename)[0]\n",
        "\n",
        "            # Open and read the file\n",
        "            file_content = extract_text_from_file(file_path)\n",
        "\n",
        "            # Split the text into chunks\n",
        "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "            docs = text_splitter.split_text(file_content)\n",
        "\n",
        "            for i, chunk in enumerate(docs):\n",
        "                # Define metadata for each chunk (you can customize this)\n",
        "                metadata = {\n",
        "                    \"File Name\": file_name_without_extension,\n",
        "                    \"Chunk Number\": i + 1,\n",
        "                }\n",
        "\n",
        "                # Create a header with metadata and file name\n",
        "                header = f\"File Name: {file_name_without_extension}\\n\"\n",
        "                for key, value in metadata.items():\n",
        "                    header += f\"{key}: {value}\\n\"\n",
        "\n",
        "                # Combine header, file name, and chunk content\n",
        "                chunk_with_header = header + file_name_without_extension + \"\\n\" + chunk\n",
        "                all_docs.append(chunk_with_header)\n",
        "    return all_docs"
      ],
      "metadata": {
        "id": "EKunZIOg52AC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the model:\n",
        "**\"llama-2-7b-chat.Q4_K_M.gguf\":** This specifies the version of the model to download. The \"Q4_K_M\" indicates a quantized model using the Q4 format, a technique that reduces the precision (e.g., from 16-bit floating point to 4-bit), making the model faster and more efficient, especially for inference on lower-resource devices"
      ],
      "metadata": {
        "id": "y7MPbstxCw84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loading_model():\n",
        "    repo_id = \"TheBloke/Llama-2-7B-Chat-GGUF\"\n",
        "    filename = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "\n",
        "    # Download the model file from Hugging Face Hub\n",
        "    local_model_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "    return local_model_path"
      ],
      "metadata": {
        "id": "xJEOM7-y6So7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Prompt Template:\n",
        "* **Prompt Template Initialization:**\n",
        "  A template is created for generating questions and answers. The initial template is a simple question-answer format, while a more complex template is created later for generating the final response.\n",
        "\n",
        "* **Model Loading:**\n",
        "The LlamaCpp model is initialized with specified parameters, allowing it to generate responses based on the context provided.\n",
        "* **Document Search:**\n",
        "The function includes a placeholder for a query (query = \"<< user question goes here >>?\"), which should be replaced with the actual question from the user.\n",
        "* **Search Context:**\n",
        "The function searches for relevant documents based on the user query and retrieves the top 5 semantically similar chunks.\n",
        "* **Final Prompt Formatting:**\n",
        "A new prompt is generated that combines the context obtained from the search results and the user's question to elicit a specific answer from the model.\n",
        "* **Response Generation:**\n",
        "The llm_chain.invoke(final_prompt) method is called to generate the response based on the final formatted prompt."
      ],
      "metadata": {
        "id": "QIzFqGPJHDlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def answer_template(question):\n",
        "    # Template for question-answer promp\n",
        "    template = '''Question: {question} \\n\\nAnswer:'''\n",
        "    # Initialize prompt template and callback manager\n",
        "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "    # Define the local path to the Llama2 model download\n",
        "    model_path = \"/root/.cache/huggingface/hub/models--TheBloke--Llama-2-7B-Chat-GGUF/snapshots/191239b3e26b2882fb562ffccdd1cf0f65402adb/llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "\n",
        "    # Initialize LlamaCpp model\n",
        "    llm = LlamaCpp(model_path=model_path, temperature=0.3, max_tokens=2047, top_p=1, callback_manager=callback_manager, n_ctx=5000)\n",
        "    # Create LLMChain\n",
        "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "    # Define a query to search the indexed documents\n",
        "    query = \"<<user question goes here>>?\"\n",
        "    # Search for semantically similar chunks and return the top 5 chunks\n",
        "    search = db.similarity_search(query, k=5)\n",
        "\n",
        "    # Define a template for generating a final prompt\n",
        "    template = '''Context: {context}\n",
        "    Based on the Context, please answer the following question:\n",
        "    Question: {question}\n",
        "    Provide an answer based on the context only, without using general knowledge. The answer\n",
        "    should be derived directly from the context provided.\n",
        "    Please correct any grammatical errors for improved readability.\n",
        "    If the context does not contain relevant information to answer the question, state that the\n",
        "    answer is not available in the given context.\n",
        "    Please include the source title of the information as a reference of how you arrive at your\n",
        "    answer. '''\n",
        "\n",
        "    # Create a prompt template\n",
        "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
        "\n",
        "    # Format the final prompt with the query and search results\n",
        "    final_prompt = prompt.format(question=query, context=search)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Query the index with the user's question\n",
        "    search = db.similarity_search(user_query, k=5)\n",
        "    context = \"\\n\".join([doc.page_content for doc in search]) if search else \"No relevant context found.\"\n",
        "    final_prompt = prompt.format(question=user_query, context=context)\n",
        "    response = llm_chain.invoke(final_prompt)\n",
        "    return response"
      ],
      "metadata": {
        "id": "pg9xD5Y06iJj"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and process PDF or text files\n",
        "\n",
        "\n",
        "def process_and_answer(files, question):\n",
        "    # Load all documents'''\n",
        "\n",
        "    all_docs=process(files, question)\n",
        "    db = FAISS.from_texts(all_docs, hf_embedding)\n",
        "\n",
        "    # Save the indexed data locally\n",
        "    db.save_local(\"faiss_AiDoc\")\n",
        "\n",
        "    # Added allow_dangerous_deserialization=True to allow loading the pickle file\n",
        "    db = FAISS.load_local(\"faiss_AiDoc\", embeddings=hf_embedding, allow_dangerous_deserialization=True)\n",
        "\n",
        "\n",
        "    local_model_path=loading_model()\n",
        "\n",
        "    response = answer_template(question)\n",
        "    return response\n",
        "    return 'Based on the context provided, sound is defined as a form of energy that produces a sensation of hearing in our ears. It is produced by vibrating objects, which sets the particles of the medium around it vibrating. These vibrations create a series of compressions and rarefactions in the air, which make up the sound wave. The compression occurs when the particles are pushed together, creating a region of high pressure, while the rarefaction happens when the particles move apart, creating a region of low pressure. This movement of particles creates the sound wave that we hear. Therefore, the answer to the question is: Sound is produced by vibrating objects in a medium, which sets the particles of the medium around it vibrating, creating a series of compressions and rarefactions in the air that make up the sound wave.'"
      ],
      "metadata": {
        "id": "nHNr9Bj1HrQ-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ids448DyOlDU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Interface using Gradio:"
      ],
      "metadata": {
        "id": "4BHerzk7H78I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Gradio interface\n",
        "def create_interface():\n",
        "    file_input = gr.File(label=\"Upload PDF or TXT\", file_types=[\"pdf\", \"txt\"])\n",
        "    user_query = gr.Textbox(label=\"Enter your question\")\n",
        "    output_text = gr.Textbox(label=\"Answer\")\n",
        "\n",
        "    # Create the interface\n",
        "    # Gradio interface for multiple file uploads and question input\n",
        "    with gr.Blocks() as demo:\n",
        "        file_input = gr.File(label=\"Upload PDF or TXT\", file_types=[\"pdf\", \"txt\"],file_count=\"multiple\")\n",
        "        question_input = gr.Textbox(label=\"Enter your question\")\n",
        "        output = gr.Textbox(label=\"Answer\")\n",
        "\n",
        "        # Add button for submission\n",
        "        submit_btn = gr.Button(\"Submit\")\n",
        "\n",
        "        # Link the inputs to the processing function\n",
        "        submit_btn.click(process_and_answer, inputs=[file_input, question_input], outputs=output)\n",
        "\n",
        "    # Launch the Gradio interface\n",
        "    demo.launch()\n",
        "# Start the Gradio interface\n",
        "if __name__ == \"__main__\":\n",
        "    create_interface()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "xrLrUTbZ28VL",
        "outputId": "39213814-00b7-40a3-843c-f627047d7949"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://08f496ca8153155db2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://08f496ca8153155db2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To run in console:"
      ],
      "metadata": {
        "id": "KtdhbDfLJgs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing documents to process\n",
        "directory = r'/content/'\n",
        "\n",
        "\n",
        "# Parameters for text splitting\n",
        "chunk_size = 1200\n",
        "chunk_overlap = 300\n",
        "\n",
        "# List to store all document chunks\n",
        "all_docs = []\n",
        "allowed_extensions = ['.pdf', '.txt', '.docx']\n",
        "\n",
        "# Process each file in the directory\n",
        "for root, dirs, files in os.walk(directory):\n",
        "    for filename in files:\n",
        "        # Get the file extension\n",
        "        _, file_extension = os.path.splitext(filename)\n",
        "        if file_extension in allowed_extensions:\n",
        "            file_path = os.path.join(root, filename)  # Full path of the file\n",
        "\n",
        "            # Remove the \".docx\", \".pdf\" or \".txt\" extension from the file name\n",
        "            file_name_without_extension = os.path.splitext(filename)[0]\n",
        "\n",
        "            # Open and read the file\n",
        "            file_content = extract_text_from_file(file_path)\n",
        "\n",
        "            # Split the text into chunks\n",
        "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "            docs = text_splitter.split_text(file_content)\n",
        "\n",
        "            for i, chunk in enumerate(docs):\n",
        "                # Define metadata for each chunk (you can customize this)\n",
        "                metadata = {\n",
        "                    \"File Name\": file_name_without_extension,\n",
        "                    \"Chunk Number\": i + 1,\n",
        "                }\n",
        "\n",
        "                # Create a header with metadata and file name\n",
        "                header = f\"File Name: {file_name_without_extension}\\n\"\n",
        "                for key, value in metadata.items():\n",
        "                    header += f\"{key}: {value}\\n\"\n",
        "\n",
        "                # Combine header, file name, and chunk content\n",
        "                chunk_with_header = header + file_name_without_extension + \"\\n\" + chunk\n",
        "                all_docs.append(chunk_with_header)\n",
        "\n",
        "            print(f\"Processed: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjmwTsPVJIJJ",
        "outputId": "ad9b6ac6-77a5-4407-da25-7dce23ad2f48"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed: iesc111.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  user_query = input(\"Ask a question (or type 'exit' to quit): \")\n",
        "  if user_query.lower() == 'exit':\n",
        "    break\n",
        "\n",
        "  search = db.similarity_search(user_query, k=5)\n",
        "  final_prompt = prompt.format(question=user_query, context=search)\n",
        "  llm_chain.invoke(final_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etPU-nxD6H4m",
        "outputId": "91493810-4405-4990-8c0a-59989deb7bd1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question (or type 'exit' to quit): What is sound and how is it produced\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 767 prefix-match hit, remaining 1332 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Based on the context provided, sound is defined as a form of energy that produces a sensation of hearing in our ears. It is produced by vibrating objects, which sets the particles of the medium around it vibrating. These vibrations create pressure variations in the medium, which propagate through the medium as sound waves.\n",
            "Therefore, the answer to the question \"What is sound and how is it produced?\" based on the context provided is:\n",
            "Sound is a form of energy that produces a sensation of hearing in our ears. It is produced by vibrating objects, which sets the particles of the medium around it vibrating, creating pressure variations in the medium as sound waves."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time = 1375648.07 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /  1332 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /   144 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time = 1075368.99 ms /  1476 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}